model:
  src_vocab_size: 4                                # Vocabulary size for source language
  tgt_vocab_size: 4                                # Vocabulary size for target language
  d_model: 128                                     # Dimension of the model embedding
  num_heads: 4                                     # Number of attention heads
  d_ff: 256                                        # Dimension of the feedforward network
  num_encoder_layers: 1
  num_decoder_layers: 1
  max_len: 128                                     # Maximum sequence length
  dropout: 0                                       # Dropout rate
  architecture: 'encoder_decoder'                  # encoder_decoder, encoder_only, decoder_only
  is_sinusoidal: false                             # Whether to use sinusoidal positional encoding

training:
  batch_size: 32
  num_epochs: 150     
  learning_rate: 0.0001
  weight_decay: 0.0                                 # Weight decay for optimizer
  cache_dir: 'cache'                                # Directory to store training logs and models
  seed: 42                                          # Random seed for reproducibility

data:
  eval_size: 10000                                  # maximum number of evaluation samples
  n: 13                                             # Dyck words semilength
  data_path: 'data/dyck_data_13.pkl'                # path to save/load the dataset
  force_regenerate: false                           # Set to true to force data regeneration
  seed: 42                                          # Random seed for reproducibility

device:
  use_cuda: true