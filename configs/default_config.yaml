model:
  src_vocab_size: 4                                # Dafault: 4. Vocabulary size for source language
  tgt_vocab_size: 4                                # Dafault: 4. Vocabulary size for target language
  d_model: 128                                     # Dafault: 128. Dimension of the model embedding
  num_heads: 4                                     # Dafault: 4. Number of attention heads
  d_ff: 256                                        # Dafault: 256. Dimension of the feedforward network
  num_encoder_layers: 1                            # Dafault: 1. Number of encoder layers
  num_decoder_layers: 1                            # Dafault: 1. Number of decoder layers
  max_len: 128                                     # Dafault: 128. Maximum sequence length
  dropout: 0                                       # Dafault: 0. Dropout rate
  architecture: 'encoder_decoder'                  # Dafault: 'encoder_decoder'. Model architecture
  is_sinusoidal: false                             # Dafault: false. Whether to use sinusoidal positional encoding
  seed: 5

training:
  batch_size: 32
  num_epochs: 33     
  learning_rate: 0.0001
  weight_decay: 0.0                                 # Dafault: 0.0. Weight decay for optimizer
  cache_dir: 'cache'                                # Dafault: 'cache'. Directory to store training logs and models
  seed: 5                                           # Dafault: 42. Random seed for reproducibility

data:
  eval_size: 10000                                  # Dafault: 10000. maximum number of evaluation samples
  n: 13                                             # Dafault: 13. Dyck words semilength
  data_path: 'data/dyck_data_13.pkl'                # Dafault: 'data/dyck_data_13.pkl'. path to save/load the dataset
  force_regenerate: false                           # Dafault: false. Set to true to force data regeneration
  seed: 42                                          # Dafault: 42. Random seed for reproducibility

device:
  use_cuda: true